# Server
PORT=3000

# LLM Provider - choose one: 'ollama', 'openai', 'anthropic'
LLM_PROVIDER=ollama

# Ollama Configuration (if using ollama)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=codellama 

