# Server
PORT=3000
LOG_LEVEL=info

# LLM Provider - choose one: 'ollama', 'openai', 'anthropic'
LLM_PROVIDER=ollama

# Ollama Configuration (if using ollama)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=codellama 

# Smee.io for local development
WEBHOOK_PROXY_URL=https://smee.io/YOUR_CHANNEL